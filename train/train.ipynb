{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://tc-radare2-training-data/radare2_train.jsonl to ./radare2_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://tc-radare2-training-data/radare2_train.jsonl .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=0\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0804b72258402e810142487974a741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import datasets\n",
    "dataset = datasets.load_dataset(\"json\", data_files=\"radare2_train.jsonl\", split=\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47f97452b8e4e859ffa1123cf8f7010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/45285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "items = []\n",
    "for index, item in enumerate(dataset):\n",
    "  if index >= 36160:    \n",
    "    for i in range(25):\n",
    "      items.append({\n",
    "        \"messages\": [\n",
    "          {\"role\": \"system\", \"content\": \"You are an expert reverse engineer, proficient in radare2.\"},\n",
    "          {\"role\": \"user\", \"content\": item['messages'][1]['content']},\n",
    "          {\"role\": \"assistant\", \"content\": item[\"messages\"][2][\"content\"]}\n",
    "        ]\n",
    "      })\n",
    "  else:\n",
    "    items.append({ \"messages\": [\n",
    "      {\"role\": item['messages'][0]['role'], \"content\": item['messages'][0]['content']},\n",
    "      {\"role\": item['messages'][1]['role'], \"content\": item['messages'][1]['content']},\n",
    "      {\"role\": item['messages'][2]['role'], \"content\": item['messages'][2]['content']}\n",
    "    ] })\n",
    "combined = datasets.Dataset.from_list(items)\n",
    "combined = combined.shuffle(seed=42)\n",
    "combined.save_to_disk('r2_dataset')\n",
    "dataset = combined\n",
    "\n",
    "# good_dataset = datasets.Dataset.from_list(items)\n",
    "# good_dataset = good_dataset.shuffle(seed=42)\n",
    "# items = []\n",
    "# for item in dataset:\n",
    "#   messages = []\n",
    "#   for m in item['messages']:\n",
    "#     messages.append({\"role\": m['role'], \"content\": m['content']})\n",
    "#   items.append({\"messages\": messages})\n",
    "\n",
    "# for item in good_dataset:\n",
    "#   messages = []\n",
    "#   for m in item['messages']:\n",
    "#     messages.append({\"role\": m['role'], \"content\": m['content']})\n",
    "#   items.append({\"messages\": messages})\n",
    "  \n",
    "# both_dataset = datasets.Dataset.from_list(items)\n",
    "# both_dataset.save_to_disk('r2_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "dataset = datasets.load_from_disk('r2_dataset')\n",
    "# print the first 10 examples\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "print(dataset)\n",
    "for i in range(10):\n",
    "  print(tokenizer.apply_chat_template(dataset[i]['messages'], tokenize=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280f405bd2d347989c53d58c04d504fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e09c954645f4d908ad03813d25c7f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openhermes.json:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d1185a31d84d88b10e32d3cfd82d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/242831 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf604b056b64058b632f6990910aca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51285 45285 6000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73da76df1c341dda64b49830122e1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/51285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "tech_dataset = datasets.load_dataset(\"teknium/openhermes\")\n",
    "\n",
    "# Random sample 6k\n",
    "random_indices = random.sample(range(len(tech_dataset['train'])), 6000)\n",
    "tech_sample = tech_dataset['train'].select(random_indices)\n",
    "\n",
    "# Convert OpenHermes format to match the existing dataset structure\n",
    "def convert_openhermes_format(example):\n",
    "    return {\n",
    "        'messages': [\n",
    "            {\"role\": \"user\", \"content\": example['instruction']},\n",
    "            {\"role\": \"assistant\", \"content\": example['output']}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Convert format\n",
    "tech_sample = tech_sample.map(convert_openhermes_format)\n",
    "\n",
    "# Ensure the structure matches between datasets\n",
    "def restructure_messages(example):\n",
    "    return {\n",
    "        'messages': [\n",
    "            {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "            for message in example['messages']\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Apply restructuring to both datasets\n",
    "combined = []\n",
    "for d in dataset: \n",
    "  combined.append(restructure_messages(d))\n",
    "for d in tech_sample: \n",
    "  combined.append(restructure_messages(d))\n",
    "print(len(combined), len(dataset), len(tech_sample))\n",
    "\n",
    "combined_dataset = datasets.Dataset.from_list(combined)\n",
    "# Shuffle the combined dataset\n",
    "combined_shuffled = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "# Save the combined dataset\n",
    "combined_shuffled.save_to_disk('r2_dataset_hermes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 42525\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.load_from_disk('r2_dataset_hermes')\n",
    "#datasets.load_from_disk('r2_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tenasorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "Train samples: 46156, Eval samples: 5129\n",
      "Train samples: 46156, Eval samples: 5129\n",
      "Train samples: 46156, Eval samples: 5129\n",
      "Train samples: 46156, Eval samples: 5129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/ubuntu/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/7e27bd9f95328f0f3b08261d1252705110c806f8/vocab.json\n",
      "loading file merges.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/7e27bd9f95328f0f3b08261d1252705110c806f8/merges.txt\n",
      "loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/7e27bd9f95328f0f3b08261d1252705110c806f8/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/7e27bd9f95328f0f3b08261d1252705110c806f8/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/7e27bd9f95328f0f3b08261d1252705110c806f8/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/7e27bd9f95328f0f3b08261d1252705110c806f8/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"HuggingFaceTB/SmolLM2-135M-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers.js_config\": {\n",
      "    \"kv_cache_dtype\": {\n",
      "      \"fp16\": \"float16\",\n",
      "      \"q4f16\": \"float16\"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/7e27bd9f95328f0f3b08261d1252705110c806f8/model.safetensors\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at HuggingFaceTB/SmolLM2-135M-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/7e27bd9f95328f0f3b08261d1252705110c806f8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 46156\n",
      "Number of epochs: 100\n",
      "Per device batch size: 16\n",
      "Gradient accumulation steps: 2\n",
      "Total batch size: 128\n",
      "Total optimization steps: 36059\n",
      "trainable params: 3,686,400 || all params: 138,201,408 || trainable%: 2.6674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "11/04/2024 13:03:23 - INFO - Formatting dataset with chatml format\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c671a32a834e8793aafd9faff54c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/04/2024 13:03:23 - INFO - Formatting dataset with chatml format\n",
      "11/04/2024 13:03:23 - INFO - Formatting dataset with chatml format\n",
      "11/04/2024 13:03:23 - INFO - Formatting dataset with chatml format\n",
      "[rank2]:[W1104 13:03:23.613081682 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[rank3]:[W1104 13:03:23.624353464 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[rank1]:[W1104 13:03:23.629782361 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[rank0]:[W1104 13:03:24.541334732 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 7,170\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 5,600\n",
      "  Number of trainable parameters = 3,686,400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='687' max='5600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 687/5600 17:39 < 2:06:37, 0.65 it/s, Epoch 12.14/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.780000</td>\n",
       "      <td>1.721320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.362600</td>\n",
       "      <td>1.348298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.198000</td>\n",
       "      <td>1.185613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.034800</td>\n",
       "      <td>1.040164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.921300</td>\n",
       "      <td>0.945929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.869600</td>\n",
       "      <td>0.904068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.844900</td>\n",
       "      <td>0.891724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ==="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"To list basic blocks in radare2, you can use the `list` function. Here's an example:\\n\\n```\\nprint(list(radare2.basic_blocks))\\n```\\n\\nThis will print a list of all the basic blocks in radare2.\\n\\nAlternatively, you can use the `list` function with a list comprehension:\\n\\n```\\nprint(list(radare2.basic_blocks, list(radare2.basic_blocks)))\\n```\\n\\nThis will print a list of all the basic blocks in radare2, along with their names.\\n\\nNote\"}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursion is a programming technique where a function calls itself repeatedly until it reaches a base case that stops the recursion. This allows the function to solve a problem by breaking it down into smaller instances of the same problem, which can be solved by the same function.\\n\\nIn other words, a recursive function calls itself with a smaller input until it reaches a base case that stops the recursion. This allows the function to solve the problem by breaking it down into smaller instances of the same problem, which can be solved by the same function.\\n\\nRecursion is a powerful technique in programming, as it allows for the creation'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"To view strings in a binary, you can use the `str.decode()` method. Here's an example:\\n\\n```python\\nimport numpy as np\\n\\n# Create a numpy array\\narr = np.array([1, 2, 3])\\n\\n# Convert the array to a binary\\nbinary_arr = arr.decode()\\n\\n# Print the binary\\nprint(binary_arr)\\n```\\n\\nThis will output:\\n\\n```\\n1 2 3\\n```\\n\\nAlternatively, you can use the `bin()` function to convert the array to a binary:\\n\\n```python\"}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ===\n",
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': '\\nThis command lists the basic blocks in radare2, which are the basic blocks of a program.\\n\\n```\\nb\\n```'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursion is a programming technique where a function calls itself to solve a problem. It is a fundamental concept in programming that allows a function to be repeated multiple times, often with the same inputs, to solve a problem.\\n\\nIn recursion, a function calls itself with a smaller input, which is the base case, and then recursively calls itself with the larger input. This process continues until the base case is reached, at which point the recursion stops and the function solves the problem.\\n\\nRecursion is useful for solving problems that have a recursive structure, such as tree traversals, recursive algorithms, and data'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': '\\nThis command allows you to view strings in a binary, which is useful for debugging and analyzing binary data.\\n\\n```\\n/s\\n```'}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ==="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nTo list basic blocks in radare2, we need to use the 'b' command which is specifically designed for basic block analysis. The 'b' command is used to list basic blocks, which are blocks of code that are executed by the assembler.\\n\\nThe basic block analysis command is used to list all the basic blocks that are executed by the assembler. This is useful for understanding the basic blocks that are executed by the assembler.\\n\\n\\n```\\nb\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursion is a programming technique where a function calls itself repeatedly until it reaches a base case that stops the recursion. This allows the function to perform repeated calculations or operations until it reaches a stopping condition.\\n\\nIn programming, recursion is used to solve problems that involve repeated calculations or computations. It is particularly useful when dealing with algorithms, data structures, and mathematical problems that involve repeated operations.\\n\\nThe basic idea of recursion is to define a function that performs a series of operations or calculations, and then call it with a smaller input or a modified version of the original input. This allows the function to perform repeated calculations'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nTo view strings in a binary, we need to use the's' command which is specifically designed for strings. The's' command is used to view strings in a binary, which is a common operation in radare2.\\n\\nThe's' command is used to view strings in a binary, which is a common operation in radare2.\\n\\n\\n```\\ns\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ==="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nTo list basic blocks in radare2, we need to use the 'b' command which is specifically designed for listing basic blocks. This is a common operation when working with binary analysis or basic block analysis. The basic block command is used to list all the basic blocks in the current session, which is essential for understanding the basic structure of the program.\\n\\n\\n```\\nb\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursion is a programming technique where a function or a block of code is repeated until it reaches a base case or a termination condition. This is achieved by using a recursive function or a loop that continues until the base case is reached.\\n\\nIn recursion, the base case is the condition that must be met before the function starts running. This is the point where the function stops and starts execution.\\n\\nFor example, consider a function that calculates the factorial of a number. If the number is 0 or 1, the function will stop and return 1. If the number is 2, the function will'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen dealing with binary analysis, strings are a fundamental data type that can be easily accessed and manipulated. The question asks about strings in a binary, which is a common operation in radare2. Since strings are binary data, we need a command that deals with binary operations. The 'b' command family handles binary operations, and since we're specifically looking for strings, we need a subcommand that deals with strings.\\n\\n\\n```\\nb\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ==="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen working with binary analysis, one of the most fundamental tasks is finding basic blocks - the basic parts of the code that perform basic operations like function calls, loops, and conditional statements. The question asks for listing basic blocks, which is a fundamental operation in radare2.\\n\\nLooking at the command structure, we need a listing operation ('l' commands) which are the base commands for analysis. The question specifically asks for basic blocks, so we need the 'b' command which is specifically for basic blocks.\\n\\n\\n```\\nb\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursion is a programming technique where a function or a block of code is repeated until it reaches a base case or a termination condition. This is done to solve problems by breaking them down into smaller, more manageable parts, and then solving each part until the solution is found.\\n\\nIn recursion, a function or a block of code is divided into smaller parts, and each part is called recursively until it reaches a base case or a termination condition. This allows the function or block of code to handle the recursion and eventually reach the base case or termination condition.\\n\\nFor example, consider a function that calculates the factor'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen working with binary analysis, strings are often the most important data structures. To get a list of strings in the binary, we need a way to view strings. The question asks about strings in a binary, so we need a command that deals with strings in a binary context. Looking at the help, we can see that's' is the string command that shows strings in the binary. Since strings are the most important data structure in a binary, we'll want to use that name.\\n\\n\\n```\\n's\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to SmolLM2-135M-r2ai/checkpoint-300\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/7e27bd9f95328f0f3b08261d1252705110c806f8/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"HuggingFaceTB/smollm2-135M-8k-lc100k-mix1-ep2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers.js_config\": {\n",
      "    \"kv_cache_dtype\": {\n",
      "      \"fp16\": \"float16\",\n",
      "      \"q4f16\": \"float16\"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "tokenizer config file saved in SmolLM2-135M-r2ai/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in SmolLM2-135M-r2ai/checkpoint-300/special_tokens_map.json\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ===\n",
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen working with binary analysis, you often need to see the basic blocks of the program. The question asks for listing, which means we need a command that can show you the basic blocks. Looking at the command set, 'ab' (analyze basic blocks) is the logical command to show basic blocks in radare2.\\n\\n\\n```\\nab\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursive programming involves the creation of a function or a block of code that calls itself repeatedly until it reaches a base case or a termination condition. This process allows for the creation of functions that can be repeated until a certain condition is met, which is useful for solving problems that involve repeated calculations or repeated operations.\\n\\nIn recursion, the base case is the condition that must be met before the function can continue to execute. This base case is often the smallest possible condition that must be met before the function can continue to execute.\\n\\nFor example, in a function that calculates the factorial of a number, the base case'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen dealing with binary analysis, strings are often the most fundamental data types. To get a list of strings in the binary, we need a command that deals with strings in radare2. Looking at the help, we find's' which is the string command. Since we want a list of strings, we just need to add's' followed by the number of strings.\\n\\n\\n```\\ns 10\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ==="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen working with reverse engineering in radare2, you often need to see the basic blocks of the code. Since this is a command-based operation, it should be started with 'b' to indicate analysis. Within the command, 'bfu' is used for analyzing basic blocks, and since we want to list them, 'l' is the natural choice. This will give you a comprehensive view of the basic blocks in the current function.\\n\\n\\n```\\nbfu\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursive programming involves the creation of a function or a procedure that performs a series of operations or steps, which are then repeated until the base case is reached. This allows for the creation of complex and recursive algorithms that can handle multiple levels of recursion.\\n\\nIn recursion, the base case is the termination condition, which is the point at which the recursion stops. This is typically the last instruction in a function or a loop, and it serves as a termination condition.\\n\\nRecursion is a powerful technique in programming that can be used for various purposes, such as:\\n\\n1. Creating recursive functions: Re'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': '\\nWhen dealing with binary analysis, strings are fundamental data structures that need to be displayed. Since we\\'re dealing with a screen view, we\\'ll need a screen command that deals with screens. Looking through the command set, we can see \\'cs\\' is the base command we need. The line after it specifically mentions \"show strings in binary\" which indicates we want to display all strings in the binary.\\n\\n\\n```\\n\\'cs showstrings\\n```'}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ==="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen working with reverse engineering in radare2, you often need to see the basic blocks of the code, which are the first few instructions that a function might contain. This is similar to how C strings work in other programming languages, where each basic block typically contains a single instruction. To see this information, I need to look at basic blocks, and specifically what they contain. The question asks for listing, which means using the 'b' command family. Since we want information about basic blocks, we should look at the info commands that deal with basic blocks, and specifically our current configuration with the 'b=' command.\\n\\n\"}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursion is a programming technique where a function calls itself to solve a problem or solve a problem in a higher-order function. This is similar to the process of recursion in mathematics, where a function calls itself to solve a problem in a higher-order function.\\n\\nIn recursion, a function calls itself to solve a problem in a lower-order function. This is similar to the process of recursion in mathematics, where a function calls itself to solve a problem in a higher-order function.\\n\\nFor example, consider the Fibonacci sequence: 0, 1, 1, 2, '}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen dealing with binaries, strings are fundamental data structures that need to be analyzed. The question asks about seeing strings, and in radare2, we need a command that can display strings in memory. Looking at the command reference, we find 'cs' is the base command for strings in radare2, and since we specifically want seeing strings, we need a modifier ('s' being specifically string), so'seeingstrings' is the perfect combination to list all strings in memory.\\n\\n\\n```\\n seeingstrings\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ==="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen working with reverse engineering in radare2, you often need to see how different parts of the code are executed. The question asks for listing basic blocks, which suggests we need a command that deals with basic block operations ('ab' in radare2). Looking at the available commands, 'ab' is specifically designed for basic block listing. This would beISTORY-in-the-career mode, which shows all basic blocks in a structured format.\\n\\n\\n```\\nab\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursion is a programming technique where a function calls itself to solve a problem or perform a series of operations. It is a fundamental concept in programming that allows for the creation of recursive functions that can be repeated until the base function reaches a certain point.\\n\\nFor example, consider a function that calculates the factorial of a number:\\n\\n```\\nfactorial(n) = n * (n + 1)\\n```\\n\\nThis function calls itself with the argument `n`, which is the base number to calculate the factorial of. The function then adds 1 to the result and repeats the process until it reaches'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen dealing with binaries, strings are fundamental data structures that can be displayed at runtime. We need a command that deals with strings in radare2. Looking at the command reference, we can see that 'cs' is the base command for strings in radare2, and adding 'i' specifically tells us we want to view/show strings.\\n\\n\\n```\\nics\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ===\n",
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen working with reverse engineering in radare2, understanding the structure of basic blocks is crucial for analysis. There are two main ways to see basic blocks:\\n\\n1. Direct comparison (d) with other commands (x)\\n2. Viewing a zoomed view of basic blocks\\n\\nThe most direct way to show basic blocks is through the d comparison command, which is what we need. This is because:\\n\\n- It's the most direct way to analyze basic blocks\\n- It shows all basic blocks in the code\\n- It's the most common and accessible command to use\\n- Many commands like x and z\"}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursion is a programming technique where a function calls itself to solve a problem or perform a series of operations. It is a fundamental concept in programming that allows the function to continue executing until it reaches a termination condition or reaches a certain state.\\n\\nIn other words, recursion involves a function calling itself, which then calls itself again until it reaches a termination condition or reaches a certain state. This is in contrast to iteration, which does not have a termination condition.\\n\\nFor example, consider a function that calculates the factorial of a number:\\n\\n```\\nfactorial(n) = n * (n/1'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen dealing with binaries, strings are fundamental data structures that can be viewed as they are composed of strings. To view these strings, I need a command that will display these binary-related data. Looking through the command list, I see 'd' is for directories and 'dq' is for querying strings, which falls into this category. The's' parameter in 'dq' is exactly what I need to show strings in my binary.\\n\\n\\n```\\ndqs\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to SmolLM2-135M-r2ai/checkpoint-600\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M-Instruct/snapshots/7e27bd9f95328f0f3b08261d1252705110c806f8/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"HuggingFaceTB/smollm2-135M-8k-lc100k-mix1-ep2\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers.js_config\": {\n",
      "    \"kv_cache_dtype\": {\n",
      "      \"fp16\": \"float16\",\n",
      "      \"q4f16\": \"float16\"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "tokenizer config file saved in SmolLM2-135M-r2ai/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in SmolLM2-135M-r2ai/checkpoint-600/special_tokens_map.json\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ==="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen working with reverse engineering in radare2, we often need to see how different parts of the code are executed. The question asks for listing basic blocks, which suggests we need a command that deals with basicice analysis and specifically listing basic blocks. Looking at the commands, 'ab' is the 'analysis' command that shows up throughout the r2 environment under 'analyt'. Since we're already showing up in a specific location, we just need to use that name as if you were in an r2 session.\\n\\n\\n```\\nab\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursion is a programming technique where a function calls itself repeatedly until it reaches a base case, which stops the function from calling itself indefinitely. This allows the function to run until it reaches a condition that stops the function from calling itself.\\n\\nFor example, in a function that calculates the factorial of a number, recursion would be used to calculate the factorial until it reaches a base case (usually 1) that stops the function from calling itself.\\n\\nRecursion is a powerful technique in programming that allows for more efficient and predictable code execution. It can be used to solve problems that would otherwise be impossible to'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen looking for strings in a binary, we need a way to see its string data. The question asks for showing strings, which is a string-related operation. Looking in the help, we can see that 'iS' commands on radare2 shows strings in strings format. This is fast, clear, and efficient for seeing string data in binary analysis.\\n\\n\\n```\\niS\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Custom Evaluation Examples ==="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt: How do I list basic blocks in radare2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': '\\nWhen working with reverse engineering in radare2, you often need to see the basic blocks of the code. Since this is a command-based operation and specifically handles opcode analysis, I\\'ll look at \\'ab\\' (add opcode analysis) followed by the range of basic blocks. The \\'b\\' suffix typically stands for \"basic\", and since I want to see analysis only, \\'a\\' would be the natural choice. The \\'b\\' will show analysis of basic blocks, which is the most specific and comprehensive way to understand how the code is organized.\\n\\n\\n```\\nabb\\n```'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: What is recursion in programming?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': 'Recursion is a programming technique where a function calls itself repeatedly until it reaches a base case, which stops the function from calling itself indefinitely. This allows the function to continue running until it reaches a condition that stops the function from calling itself.\\n\\nFor example, in a function that calculates the factorial of a number, recursion would be used to calculate the factorial until it reaches a base case that stops the function from calling itself.\\n\\nRecursion is a powerful technique in programming that allows for more efficient and predictable code execution. It can be used to solve problems that would otherwise be impossible to solve without recursion'}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: How do I view strings in a binary?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'role': 'assistant', 'content': \"\\nWhen dealing with binaries, strings are fundamental data structures that can be viewed as strings in human-readable format. We need a command that can handle strings in radare2. Looking at the command structure, the 'ps' subcommand deals with strings, and's' is the standard string modifier. The 'i' modifier specifically handles integers as strings. This combination will output a list of strings in a human-readable format.\\n\\n\\n```\\ni si\\n```\"}\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 799\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import torch\n",
    "import logging\n",
    "import datasets\n",
    "import torch.distributed as dist\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, load_from_disk\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModel\n",
    ")\n",
    "import os\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer, setup_chat_format\n",
    "from accelerate import PartialState\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from transformers import pipeline\n",
    "\n",
    "def generate_response(model, tokenizer, messages, device=\"cuda\"):\n",
    "    \"\"\"Generate a response from the model based on an instruction.\"\"\"\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    return pipe(messages, max_new_tokens=128)[0]['generated_text'][-1]\n",
    "class CustomCallback(transformers.TrainerCallback):\n",
    "    def __init__(self, model, tokenizer, accelerator):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eval_examples = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert reverse engineer, proficient in radare2.\"},\n",
    "                {\"role\": \"user\", \"content\": \"How do I list basic blocks in radare2?\"}\n",
    "            ],\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"What is recursion in programming?\"}\n",
    "            ],\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert reverse engineer, proficient in radare2.\"},\n",
    "                {\"role\": \"user\", \"content\": \"How do I view strings in a binary?\"}\n",
    "            ]\n",
    "        ]\n",
    "        self.accelerator = accelerator\n",
    "        \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Only run on main process with accelerate\n",
    "        if self.accelerator.is_main_process:\n",
    "            print(\"\\n=== Custom Evaluation Examples ===\")\n",
    "            for messages in self.eval_examples:\n",
    "                print(f\"\\nPrompt: {messages[1]['content']}\")\n",
    "                try:\n",
    "                    response = generate_response(self.model, self.tokenizer, messages)\n",
    "                    print(f\"Response: {response}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating response: {e}\")\n",
    "                print(\"-\" * 50)\n",
    "def train():\n",
    "    # Load dataset and model\n",
    "   # Load and split dataset\n",
    "    dataset = load_from_disk('r2_dataset_hermes')\n",
    "    \n",
    "    train_dataset, eval_dataset = dataset.train_test_split(\n",
    "        test_size=0.1,\n",
    "        seed=42  # Added for reproducibility\n",
    "    ).values() \n",
    "\n",
    "\n",
    "    model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "    output_dir = f\"SmolLM2-135M-r2ai\"\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)}, Eval samples: {len(eval_dataset)}\")\n",
    "    # Calculate optimal training parameters\n",
    "    num_training_samples = len(train_dataset)\n",
    "    num_gpus = 4\n",
    "    per_device_batch_size = 16  # Reduced from 32\n",
    "    total_batch_size = per_device_batch_size * num_gpus\n",
    "    grad_accum_steps = 2  # Reduced from 4\n",
    "    effective_batch_size = total_batch_size * grad_accum_steps\n",
    "    num_epochs = 100\n",
    "    num_training_steps = (num_training_samples * num_epochs) // effective_batch_size\n",
    "    \n",
    "    # Accelerator setup\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=grad_accum_steps,\n",
    "        mixed_precision=\"bf16\",\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=\"logs\"\n",
    "    )\n",
    "    \n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    # LoRA configuration with target modules\n",
    "    peft_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        inference_mode=False,\n",
    "    )\n",
    "    def formatting_prompts_func(example: dict) -> str:\n",
    "        \"\"\"Format prompt for training.\"\"\"\n",
    "        text = f\"<|im_start|>system\\n{example['messages'][0]['content']}<|im_end|><|im_start|>user\\n{example['messages'][1]['content']}<|im_end|>\\n<|im_start|>assistant\\n{example['messages'][2]['content']}<|im_end|>\"\n",
    "        return text\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map={'':accelerator.process_index},\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    # model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "    # for name, param in model.base_model.named_parameters():\n",
    "    #     param.requires_grad = False\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    # model.enable_input_requires_grad()\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     param.requires_grad = True\n",
    "\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        print(f\"Number of training samples: {num_training_samples}\")\n",
    "        print(f\"Number of epochs: {num_epochs}\")\n",
    "        print(f\"Per device batch size: {per_device_batch_size}\")\n",
    "        print(f\"Gradient accumulation steps: {grad_accum_steps}\")\n",
    "        print(f\"Total batch size: {effective_batch_size}\")\n",
    "        print(f\"Total optimization steps: {num_training_steps}\")\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    # Training configuration\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        # max_steps=num_training_steps,\n",
    "        max_seq_length=768,\n",
    "        per_device_train_batch_size=per_device_batch_size,\n",
    "        gradient_accumulation_steps=grad_accum_steps,\n",
    "        gradient_checkpointing=False,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=300,\n",
    "        logging_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        learning_rate=0.0005,\n",
    "        weight_decay=0.0001,\n",
    "        bf16=True,\n",
    "        warmup_ratio=0.05,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        packing=True,\n",
    "        ddp_find_unused_parameters=False,\n",
    "        eval_strategy=\"epoch\",\n",
    "        # eval_steps=max(num_training_steps // 20, 1),  # Evaluate 20 times during training\n",
    "        logging_first_step=True,\n",
    "        # load_best_model_at_end=True,\n",
    "        # metric_for_best_model=\"eval_loss\",\n",
    "        remove_unused_columns=False,\n",
    "        group_by_length=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        # formatting_func=formatting_prompts_func,\n",
    "        args=sft_config,\n",
    "        callbacks=[CustomCallback(model, tokenizer, accelerator)]\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    trainer.save_model(output_dir)\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "\n",
    "    # Final evaluation\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "notebook_launcher(train, num_processes=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': \"I think there's a jump table in this function. How can I find its location?\", 'a': 'CCf~cases'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Paris is the capital of France, and it is located in the northern part of the country. It is a city that has been a major center of culture, art, and politics for centuries. Paris is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. The city is also famous for its fashion, gastronomy, and art, and it is often referred to as the \"City of Light.\"'}\n",
      "PREDICTED ANSWER:  Paris is the capital of France, and it is located in the northern part of the country. It is a city that has been a major center of culture, art, and politics for centuries. Paris is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. The city is also famous for its fashion, gastronomy, and art, and it is often referred to as the \"City of Light.\"\n",
      "CORRECT ANSWER:  Paris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"\\nTo find a jump table, we need to use the 'j' command, which is a common way to find jump tables in a binary. The 'j' command is specifically designed for finding jump tables, and it's often used to find specific instructions or data.\\n\\nThe question is asking for the location of a jump table, so we need to use the 'j' command with the 'j' command family. The 'j' command family is specifically designed for finding jump tables, and it's often used to find specific instructions or data.\\n\\n\\n```\\njj\\n```\"}\n",
      "PREDICTED ANSWER:  \n",
      "To find a jump table, we need to use the 'j' command, which is a common way to find jump tables in a binary. The 'j' command is specifically designed for finding jump tables, and it's often used to find specific instructions or data.\n",
      "\n",
      "The question is asking for the location of a jump table, so we need to use the 'j' command with the 'j' command family. The 'j' command family is specifically designed for finding jump tables, and it's often used to find specific instructions or data.\n",
      "\n",
      "\n",
      "```\n",
      "jj\n",
      "```\n",
      "CORRECT ANSWER:  CCf~cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"\\nTo find the switch table destinations, we need to analyze the binary's address space. The question asks for a way to find the destination of a specific address, which is a common operation in reverse engineering. The 'a' command is for analyzing binary data, and specifically 'a.a' is for finding the address table destinations.\\n\\n\\n```\\na.a\\n```\"}\n",
      "PREDICTED ANSWER:  \n",
      "To find the switch table destinations, we need to analyze the binary's address space. The question asks for a way to find the destination of a specific address, which is a common operation in reverse engineering. The 'a' command is for analyzing binary data, and specifically 'a.a' is for finding the address table destinations.\n",
      "\n",
      "\n",
      "```\n",
      "a.a\n",
      "```\n",
      "CORRECT ANSWER:  afb.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"\\nTo find out the type of file, we need to use the 'f' command which is specifically designed for finding file types. The question asks for finding out the type of file, and since we're looking for ELF or PE (binary file) type, we need to use the 'f' command with the 'e' (binary) option.\\n\\n\\n```\\nf e\\n```\"}\n",
      "PREDICTED ANSWER:  \n",
      "To find out the type of file, we need to use the 'f' command which is specifically designed for finding file types. The question asks for finding out the type of file, and since we're looking for ELF or PE (binary file) type, we need to use the 'f' command with the 'e' (binary) option.\n",
      "\n",
      "\n",
      "```\n",
      "f e\n",
      "```\n",
      "CORRECT ANSWER:  i~^format[1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"\\nWhen looking for loops in a function, we need to look for the 'loops' keyword. The 'loops' keyword is used to search for loops in a function. The 'l' suffix indicates that we want to find all loops.\\n\\nLooking at the command, we need to use 'l' followed by the 'loops' keyword. This is a common pattern in command-line analysis, and it's essential to use the 'l' command to find all loops in a function.\\n\\n\\n```\\nlloops\\n```\"}\n",
      "PREDICTED ANSWER:  \n",
      "When looking for loops in a function, we need to look for the 'loops' keyword. The 'loops' keyword is used to search for loops in a function. The 'l' suffix indicates that we want to find all loops.\n",
      "\n",
      "Looking at the command, we need to use 'l' followed by the 'loops' keyword. This is a common pattern in command-line analysis, and it's essential to use the 'l' command to find all loops in a function.\n",
      "\n",
      "\n",
      "```\n",
      "lloops\n",
      "```\n",
      "CORRECT ANSWER:  /abf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"\\nWhen looking for command line arguments, we need to look at the program's behavior. The question asks for finding where the program might be using command line arguments, which is a common operation in debugging and analysis. The command to find arguments is 'arg', and the question asks for 'arg' specifically. The '?' is a wildcard that matches any character, so we need to use it to find all arguments.\\n\\n\\n```\\narg?\\n```\"}\n",
      "PREDICTED ANSWER:  \n",
      "When looking for command line arguments, we need to look at the program's behavior. The question asks for finding where the program might be using command line arguments, which is a common operation in debugging and analysis. The command to find arguments is 'arg', and the question asks for 'arg' specifically. The '?' is a wildcard that matches any character, so we need to use it to find all arguments.\n",
      "\n",
      "\n",
      "```\n",
      "arg?\n",
      "```\n",
      "CORRECT ANSWER:  axt@@=reloc.optind reloc.optarg sym.imp.getopt_long sym.imp.getopt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 49\u001b[0m\n\u001b[1;32m     42\u001b[0m   eval_messages\u001b[38;5;241m.\u001b[39mappend([\n\u001b[1;32m     43\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an expert reverse engineer, proficient in radare2.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     44\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m]},\n\u001b[1;32m     45\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     46\u001b[0m   ])\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m eval_messages:\n\u001b[0;32m---> 49\u001b[0m   r \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m   \u001b[38;5;28mprint\u001b[39m(r)\n\u001b[1;32m     52\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPREDICTED ANSWER: \u001b[39m\u001b[38;5;124m'\u001b[39m, r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(model, tokenizer, messages, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a response from the model based on an instruction.\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:267\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    263\u001b[0m     text_inputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, KeyDataset) \u001b[38;5;28;01mif\u001b[39;00m is_torch_available() \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    264\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m         chats \u001b[38;5;241m=\u001b[39m [Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs]  \u001b[38;5;66;03m#   \u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/pipelines/base.py:1302\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/pipelines/base.py:1309\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1308\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1309\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/pipelines/base.py:1209\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1208\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/peft/peft_model.py:1704\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1703\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1704\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1706\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:673\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m        into the model\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    671\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    677\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    678\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    686\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:72\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     70\u001b[0m input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     71\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 72\u001b[0m variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import StoppingCriteria\n",
    "from transformers import pipeline\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops=None):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        if input_ids.shape[-1] > 1:\n",
    "            last_token = input_ids[0][-1]\n",
    "            if last_token in self.stops:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def generate_response(model, tokenizer, messages, device=\"cuda\"):\n",
    "    \"\"\"Generate a response from the model based on an instruction.\"\"\"\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
    "    return pipe(messages, max_new_tokens=128)[0]['generated_text'][-1]\n",
    "    \n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "output_dir = f\"SmolLM2-135M-r2ai\"\n",
    "model = AutoModelForCausalLM.from_pretrained(        \n",
    "  model_name,\n",
    "\n",
    "  torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "peft_model = PeftModel.from_pretrained(model, output_dir)\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='radare2_ok.tsv', split='train', sep='\\t')\n",
    "print(dataset[0])\n",
    "eval_messages = [\n",
    "  [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris\"}]\n",
    "]\n",
    "for d in dataset:\n",
    "  eval_messages.append([\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert reverse engineer, proficient in radare2.\"},\n",
    "    {\"role\": \"user\", \"content\": d['q']},\n",
    "    {\"role\": \"assistant\", \"content\": d['a']}\n",
    "  ])\n",
    "\n",
    "for m in eval_messages:\n",
    "  r = generate_response(peft_model, tokenizer, m[:2])\n",
    "  print(r)\n",
    "\n",
    "  print('PREDICTED ANSWER: ', r['content'])\n",
    "  print('CORRECT ANSWER: ', m[-1]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting datasource.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile datasource.py\n",
    "\n",
    "import datasets\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "EOT_ID = 128009\n",
    "\n",
    "def mask_target(target,seq):\n",
    "    for i in range(len(seq)-len(target)):\n",
    "        if seq[i:i+len(target)] == target:\n",
    "            seq[i:i+len(target)] = [-100] * len(target)\n",
    "    return seq\n",
    "\n",
    "def get_custom_dataset(dataset_config, tokenizer, split):\n",
    "    \n",
    "    def tokenize_function(messages):\n",
    "        dialog_tokens = tokenizer.apply_chat_template(messages)\n",
    "        eot_indices = [i for i,n in enumerate(dialog_tokens) if n == EOT_ID]\n",
    "        labels = copy.copy(dialog_tokens)\n",
    "        system_or_user = (tokenizer.encode(\"system\")[-1], tokenizer.encode(\"user\")[-1])\n",
    "        labels[0] = -100 # bos token\n",
    "        last_idx = 1\n",
    "        for n, idx in enumerate(eot_indices):\n",
    "            role_token = labels[last_idx+1]\n",
    "            if role_token in system_or_user:\n",
    "                # Set labels to -100 for system and user tokens to ignore in loss function\n",
    "                labels[last_idx:idx+1] = [-100] * (idx-last_idx+1)\n",
    "            last_idx = idx + 1\n",
    "        mask_target(tokenizer.encode(\"<|start_header_id|>assistant<|end_header_id|>\", add_special_tokens=False), labels)\n",
    "        dialog_tokens = [dialog_tokens]\n",
    "        labels_tokens = [labels]\n",
    "        combined_tokens = {\n",
    "            \"input_ids\": list(itertools.chain(*(t for t in dialog_tokens))),\n",
    "            \"labels\": list(itertools.chain(*(t for t in labels_tokens))),\n",
    "        }\n",
    "    \n",
    "        return dict(combined_tokens, attention_mask=[1]*len(combined_tokens[\"input_ids\"]))\n",
    "\n",
    "    dataset = datasets.load_from_disk('r2_dataset')\n",
    "    try:    \n",
    "        if split == 'train':\n",
    "            dataset = dataset['train']\n",
    "        else:\n",
    "            dataset = dataset['test']\n",
    "    except:\n",
    "        pass\n",
    "    dataset = dataset.map(lambda x: tokenize_function(x['messages']), remove_columns=['messages'])\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing finetuning.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile finetuning.py\n",
    "\n",
    "import fire\n",
    "from llama_recipes.finetuning import main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%env TOKENIZERS_PARALLELISM=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1103 18:39:14.630121 155400 torch/distributed/run.py:793] \n",
      "W1103 18:39:14.630121 155400 torch/distributed/run.py:793] *****************************************\n",
      "W1103 18:39:14.630121 155400 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1103 18:39:14.630121 155400 torch/distributed/run.py:793] *****************************************\n",
      "/home/ec2-user/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
      "  from torch.distributed._shard.checkpoint import (\n",
      "/home/ec2-user/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
      "  from torch.distributed._shard.checkpoint import (\n",
      "/home/ec2-user/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
      "  from torch.distributed._shard.checkpoint import (\n",
      "/home/ec2-user/llama-recipes/src/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
      "  from torch.distributed._shard.checkpoint import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU cache for all ranks\n",
      "--> Running with torch dist debug set to detail\n",
      "--> Model meta-llama/Llama-3.2-1B-Instruct\n",
      "\n",
      "--> meta-llama/Llama-3.2-1B-Instruct has 1235.8144 Million params\n",
      "\n",
      "trainable params: 2,359,296 || all params: 1,238,173,696 || trainable%: 0.1905\n",
      "bFloat16 enabled for mixed precision - using bfSixteen policy\n",
      "trainable params: 2,359,296 || all params: 1,238,173,696 || trainable%: 0.1905\n",
      "trainable params: 2,359,296 || all params: 1,238,173,696 || trainable%: 0.1905\n",
      "trainable params: 2,359,296 || all params: 1,238,173,696 || trainable%: 0.1905\n",
      "--> Training Set Length = 45285\n",
      "--> Validation Set Length = 45285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset: 100%|| 45285/45285 [00:06<00:00, 7173.13it/s]\n",
      "Preprocessing dataset: 100%|| 45285/45285 [00:06<00:00, 7140.34it/s]\n",
      "Preprocessing dataset: 100%|| 45285/45285 [00:06<00:00, 7095.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Packing Statistics:\n",
      "Total packed sequences: 9791\n",
      "Average tokens per sequence: 384.00\n",
      "Leftover tokens: 87\n",
      "length of dataset_train 9791\n",
      "Can not find the custom data_collator in the dataset.py file (datasource.py).\n",
      "Using the default data_collator instead.\n",
      "--> Num of Training Set Batches loaded = 305\n",
      "Starting epoch 0/3\n",
      "train_config.max_train_step: 0\n",
      "\n",
      "Packing Statistics:\n",
      "Total packed sequences: 9791\n",
      "Average tokens per sequence: 384.00\n",
      "Leftover tokens: 87\n",
      "length of dataset_train 9791\n",
      "Can not find the custom data_collator in the dataset.py file (datasource.py).\n",
      "Using the default data_collator instead.\n",
      "--> Num of Training Set Batches loaded = 305\n",
      "Starting epoch 0/3\n",
      "train_config.max_train_step: 0\n",
      "\n",
      "Packing Statistics:\n",
      "Total packed sequences: 9791\n",
      "Average tokens per sequence: 384.00\n",
      "Leftover tokens: 87\n",
      "length of dataset_train 9791\n",
      "Can not find the custom data_collator in the dataset.py file (datasource.py).\n",
      "Using the default data_collator instead.\n",
      "--> Num of Training Set Batches loaded = 305\n",
      "Starting epoch 0/3\n",
      "train_config.max_train_step: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset:  99%|| 45045/45285 [00:06<00:00, 7936.38it/s]/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Preprocessing dataset: 100%|| 45285/45285 [00:06<00:00, 6979.49it/s]\n",
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/305 [00:00<?, ?it/s]/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/305 [00:00<?, ?it/s]/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/305 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Packing Statistics:\n",
      "Total packed sequences: 9791\n",
      "Average tokens per sequence: 384.00\n",
      "Leftover tokens: 87\n",
      "length of dataset_train 9791\n",
      "Can not find the custom data_collator in the dataset.py file (datasource.py).\n",
      "Using the default data_collator instead.\n",
      "--> Num of Training Set Batches loaded = 305\n",
      "Starting epoch 0/3\n",
      "train_config.max_train_step: 0\n",
      "pytorch profiling is activated and results will be saved in profiler\n",
      "pytorch profiling is activated and results will be saved in profiler\n",
      "pytorch profiling is activated and results will be saved in profiler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m          \u001b[0m| 0/305 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch profiling is activated and results will be saved in profiler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1/3, step 4/305 completed (loss: 2.429292678833008):   2%|\u001b[34m         \u001b[0m| 6/305 [00:10<08:32,  1.71s/it]]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import signal\n",
    "if torchrun:\n",
    "  os.killpg(os.getpgid(torchrun.pid), signal.SIGTERM)\n",
    "  torchrun = None\n",
    "\n",
    "# Model config\n",
    "IS_LORA = True\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "name = 'r2ai-3.2-1B-Instruct'\n",
    "one_gpu = False\n",
    "# Training setup\n",
    "num_nodes = 1\n",
    "nproc_per_node = 4\n",
    "enable_fsdp = True\n",
    "low_cpu_fsdp = True\n",
    "dist_checkpoint_root_folder = \"/mnt/efs/checkpoints\"\n",
    "dist_checkpoint_folder = name\n",
    "run_validation = False\n",
    "# Auto context length based on GPU\n",
    "context_length = 384 # if torch.cuda.get_device_properties(0).total_memory < 16e9 else 2048 # T4 16GB or A10 24GB\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_clipping = True\n",
    "gradient_clipping_threshold = 1.0\n",
    "batch_size_training = 8\n",
    "max_eval_step = 0\n",
    "num_workers_dataloader = 4\n",
    "weight_decay = 0.01\n",
    "gamma = 0.85\n",
    "seed = 123\n",
    "use_fp16 = False\n",
    "pure_bf16 = True\n",
    "mixed_precision = True\n",
    "val_batch_size = 1\n",
    "dataset = \"custom_dataset\"\n",
    "freeze_layers = False\n",
    "num_freeze_layers = 1\n",
    "quantization = None\n",
    "one_gpu = False\n",
    "save_model = True\n",
    "dist_checkpoint_root_folder = \"/mnt/efs/checkpoints\"\n",
    "dist_checkpoint_folder = name\n",
    "save_optimizer = False\n",
    "use_fast_kernels = True\n",
    "use_wandb = False\n",
    "save_metrics = True\n",
    "flop_counter = False\n",
    "flop_counter_start = 3\n",
    "use_profiler = True\n",
    "profiler_dir = \"profiler\"\n",
    "fsdp_activation_checkpointing = False\n",
    "peft_method = \"lora\"\n",
    "lora_config = {\n",
    "    \"r\": 16,\n",
    "    \"alpha\": 32,\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"dropout\": 0.05,\n",
    "    \"target_modules\": 'q_proj,v_proj,k_proj',\n",
    "    \"inference_mode\": False,\n",
    "    \"bias\": \"none\"\n",
    "}\n",
    "\n",
    "from_peft_checkpoint = \"\"\n",
    "output_dir = \"./output\"\n",
    "\n",
    "# LoRA params\n",
    "if IS_LORA:\n",
    "   use_peft = True\n",
    "else:\n",
    "   use_peft = False\n",
    "   \n",
    "\n",
    "# Training params\n",
    "num_epochs = 3\n",
    "max_train_step = 0\n",
    "batching_strategy = \"packing\"\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Build command\n",
    "cmd = (\n",
    "   f\"ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 ~/.venv/bin/torchrun \"\n",
    "   f\"--nnodes {num_nodes} \"\n",
    "   f\"--nproc_per_node {nproc_per_node} \"\n",
    "   f\"finetuning.py \"\n",
    "   f\"--lr {learning_rate} \"\n",
    "   f\"--max_train_step {max_train_step} \"\n",
    "   f\"--enable_fsdp {enable_fsdp} \"\n",
    "   f\"--low_cpu_fsdp {low_cpu_fsdp} \"\n",
    "   f\"--model_name {model_name} \"\n",
    "   f\"--dist_checkpoint_root_folder {dist_checkpoint_root_folder} \"\n",
    "   f\"--dist_checkpoint_folder {dist_checkpoint_folder} \"\n",
    "   f\"--fsdp_config.pure_bf16 {pure_bf16} \"\n",
    "   f\"--use_fast_kernels \"\n",
    "   f\"--dataset custom_dataset \"\n",
    "   f\"--custom_dataset.file datasource.py \"\n",
    "   f\"--num_epochs {num_epochs} \"\n",
    "   f\"--use_peft {use_peft} \"\n",
    "   f\"--peft_method {peft_method} \"\n",
    "   f\"--lora_config.r {lora_config['r']} \"\n",
    "   f\"--lora_config.lora_alpha {lora_config['alpha']} \"\n",
    "   f\"--lora_config.lora_dropout {lora_config['dropout']} \"\n",
    "   f\"--lora_config.target_modules {lora_config['target_modules']} \"\n",
    "   f\"--lora_config.task_type {lora_config['task_type']} \"\n",
    "   f\"--lora_config.bias {lora_config['bias']} \"\n",
    "   f\"--lora_config.inference_mode {lora_config['inference_mode']} \"\n",
    "   f\"--context_length {context_length} \"\n",
    "   f\"--batching_strategy {batching_strategy} \"\n",
    "   f\"--run_validation {run_validation} \"\n",
    "   f\"--weight_decay {weight_decay} \"\n",
    "   f\"--gradient_accumulation_steps {gradient_accumulation_steps} \"\n",
    "   f\"--gradient_clipping {gradient_clipping} \"\n",
    "   f\"--gradient_clipping_threshold {gradient_clipping_threshold} \"\n",
    "   f\"--batch_size_training {batch_size_training} \"\n",
    "   f\"--num_workers_dataloader {num_workers_dataloader} \"\n",
    "   f\"--max_eval_step {max_eval_step} \"\n",
    "   f\"--val_batch_size {val_batch_size} \"\n",
    "   f\"--freeze_layers {freeze_layers} \"\n",
    "   f\"--num_freeze_layers {num_freeze_layers} \"\n",
    "   f\"--quantization {quantization} \"\n",
    "   f\"--one_gpu {one_gpu} \"\n",
    "   f\"--seed {seed} \"\n",
    "   f\"--use_fp16 {use_fp16} \"\n",
    "   f\"--mixed_precision {mixed_precision} \"\n",
    "   f\"--flop_counter {flop_counter} \"\n",
    "   f\"--flop_counter_start {flop_counter_start} \"\n",
    "   f\"--use_wandb {use_wandb} \"\n",
    "   f\"--save_metrics {save_metrics} \"\n",
    "   f\"--save_model {save_model} \"\n",
    "   f\"--save_optimizer {save_optimizer} \"\n",
    "   f\"--use_profiler {use_profiler} \"\n",
    "   f\"--profiler_dir {profiler_dir} \"\n",
    "   f\"--gamma {gamma} \"\n",
    "   f\"--fsdp_config.fsdp_activation_checkpointing {fsdp_activation_checkpointing} \"\n",
    "   f\"--fsdp_config.use_fp16 {use_fp16} \"\n",
    "   f\"--output_dir {output_dir} \"\n",
    ")\n",
    "\n",
    "# delete previous metric files\n",
    "for file in os.listdir('./output'):\n",
    "  if file.endswith('.json') and 'metrics_data' in file:\n",
    "    os.remove(f'./output/{file}')\n",
    "\n",
    "import subprocess\n",
    "torchrun = subprocess.Popen(cmd, shell=True, preexec_fn=os.setsid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3/3, step 185/305 completed (loss: 1.358014702796936):  61%|\u001b[34m   \u001b[0m| 187/305 [05:02<03:09,  1.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3/3, step 304/305 completed (loss: 1.3201574087142944): 100%|\u001b[34m\u001b[0m| 305/305 [08:12<00:00,  1.61s/it]\n",
      "Training Epoch: 3/3, step 304/305 completed (loss: 1.3307762145996094): 100%|\u001b[34m\u001b[0m| 305/305 [08:12<00:00,  1.61s/it]\n",
      "Training Epoch: 3/3, step 304/305 completed (loss: 1.1081926822662354): 100%|\u001b[34m\u001b[0m| 305/305 [08:12<00:00,  1.61s/it]\n",
      "Training Epoch: 3/3, step 304/305 completed (loss: 1.3163824081420898): 100%|\u001b[34m\u001b[0m| 305/305 [08:12<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 11 GB\n",
      "Max CUDA memory reserved was 11 GB\n",
      "Peak active CUDA memory was 11 GB\n",
      "CUDA Malloc retries : 0\n",
      "CPU Total Peak Memory consumed during the train (max): 10 GB\n",
      "we are about to save the PEFT modules\n",
      "PEFT modules are saved in ./output directory\n",
      "Epoch 3: train_perplexity=3.7166, train_epoch_loss=1.3128, epoch time 493.58159019400046s\n",
      "Key: avg_train_prep, Value: 4.3590989112854\n",
      "Key: avg_train_loss, Value: 1.460320274035136\n",
      "Key: avg_epoch_time, Value: 493.26845016233347\n",
      "Key: avg_checkpoint_time, Value: 3.8840213423330474\n",
      "Key: metrics_filename, Value: ./output/metrics_data_0-2024-11-03_18-39-28.json\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def plot_metrics(data, metric_name, x_label, y_label, title, ax):\n",
    "    x = range(len(data[f'{metric_name}']))\n",
    "    y = data[f'{metric_name}']\n",
    "    ax.plot(x, y)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "\n",
    "clear_output(wait=True)\n",
    "\n",
    "all_data = []\n",
    "metrics = []\n",
    "for i, file in enumerate([f for f in os.listdir('./output') if f.endswith('.json') and 'metrics_data' in f]):\n",
    "    data = json.load(open(f'./output/{file}'))\n",
    "    all_data.append(data)\n",
    "    for key in data.keys():\n",
    "        if key not in metrics:\n",
    "            metrics.append(key)\n",
    "\n",
    "averages = {}\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    averages[metric] = []\n",
    "    for j in range(len(all_data[0][metric])):\n",
    "        avg = np.mean([data[metric][j] for data in all_data])\n",
    "        averages[metric].append(avg)\n",
    "n = 0\n",
    "for a in averages.values():\n",
    "    if len(a) > 0:\n",
    "        n += 1\n",
    "\n",
    "# Calculate number of rows needed for 2 plots per row\n",
    "num_rows = (n + 1) // 2  # Round up division to get enough rows\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6*num_rows))\n",
    "i = 0\n",
    "for metric in metrics:\n",
    "    if len(averages[metric]) > 0:\n",
    "        i += 1\n",
    "        # Create subplot with 2 columns and calculated rows\n",
    "        ax = fig.add_subplot(num_rows, 2, i)\n",
    "        plot_metrics(averages, metric, '', '', metric + ' ' + str(averages[metric][-1]), ax)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1103 17:39:17.727921 145732 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGTERM death signal, shutting down workers\n",
      "W1103 17:39:17.728768 145732 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 145781 closing signal SIGTERM\n",
      "W1103 17:39:17.729342 145732 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 145782 closing signal SIGTERM\n",
      "W1103 17:39:17.729825 145732 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 145783 closing signal SIGTERM\n",
      "W1103 17:39:17.730362 145732 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 145784 closing signal SIGTERM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/.venv/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/distributed/launcher/api.py\", line 260, in launch_agent\n",
      "    result = agent.run()\n",
      "  File \"/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 137, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "  File \"/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 696, in run\n",
      "    result = self._invoke_run(role)\n",
      "  File \"/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 855, in _invoke_run\n",
      "    time.sleep(monitor_interval)\n",
      "  File \"/home/ec2-user/.venv/lib64/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 84, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 145732 got signal: 15\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "if torchrun:\n",
    "  os.killpg(os.getpgid(torchrun.pid), signal.SIGTERM)\n",
    "  torchrun = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/llama_recipes/model_checkpointing/checkpoint_handler.py:259: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "  dist_cp.load_state_dict(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharded state checkpoint loaded from /mnt/efs/checkpoints/r2ai-3.2-1B-Instruct-meta-llama/Llama-3.2-1B-Instruct\n",
      "HuggingFace model checkpoints has been saved in None\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from llama_recipes.inference.model_utils import  load_llama_from_config\n",
    "from llama_recipes.model_checkpointing import load_sharded_model_single_gpu\n",
    "dist_checkpoint_root_folder = \"/mnt/efs/checkpoints\"\n",
    "\n",
    "model_def = load_llama_from_config(model_name)\n",
    "model = load_sharded_model_single_gpu(model_def, dist_checkpoint_root_folder + \"/\" + 'r2ai-3.2-1B-Instruct-meta-llama/Llama-3.2-1B-Instruct')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "save_path = dist_checkpoint_root_folder + \"/\" + 'hf/' + name\n",
    "tokenizer.save_pretrained(save_path)\n",
    "hf_model_path = model.save_pretrained(save_path)\n",
    "print(f\"HuggingFace model checkpoints has been saved in {hf_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': \"I think there's a jump table in this function. How can I find its location?\", 'a': 'CCf~cases'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='radare2_ok.tsv', split='train', sep='\\t')\n",
    "print(dataset[0])\n",
    "eval_messages = [\n",
    "  [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "]\n",
    "for d in dataset:\n",
    "  eval_messages.append([\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert reverse engineer, proficient in radare2.\"},\n",
    "    {\"role\": \"user\", \"content\": d['q']}\n",
    "  ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_fast_kernelsTrue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How can I rename a flag from 'flag.old' to 'flag.new'?\n",
      "```\n",
      "fr flag.new flag.old\n",
      "```\n",
      "\n",
      "Set the calling convention of the `add` function to `cdecl`.\n",
      "```\n",
      "afcs cdecl @ sym.add\n",
      "```\n",
      "\n",
      "How can I remove all breakpoints in the debugger?\n",
      "?\n",
      "\n",
      "```\n",
      "db-\n",
      "```\n",
      "\n",
      "How can I disassemble the function at address 0x401000?\n",
      "```\n",
      "pdf @ 0x401000\n",
      "```\n",
      "\n",
      "Visualize the call graph in an interactive ASCII art view.\n",
      "This command will display the call graph in a format that is easy to read and understand, with each branch represented by a line in ASCII art.\n",
      "\n",
      "```\n",
      "agc\n",
      "```\n",
      "\n",
      "Show code locations, relationing source-line with address\n",
      ".\n",
      "\n",
      "```\n",
      "CCF\n",
      "```\n",
      "\n",
      "Find all functions calling `system`\n",
      "\n",
      "\n",
      "```\n",
      "axt @f:system\n",
      "```\n",
      "\n",
      "List functions calling imports that interact with the console, like printing text or reading from stdin\n",
      "This command lists all functions that call imports that interact with the console, such as printing text or reading from stdin.\n",
      "\n",
      "```\n",
      "ahm\n",
      "```\n",
      "\n",
      "How do I list all imported libraries?\n",
      "```\n",
      "iil\n",
      "```\n",
      "\n",
      "Temporal seek to the program counter and show the 32 bytes in hexpairs\n",
      ".\n",
      "\n",
      "```\n",
      "px 32\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from llama_recipes.inference.model_utils import load_model\n",
    "import torch\n",
    "from dataclasses import asdict\n",
    "from peft import PeftModel, LoraConfig\n",
    "hf_model_path = dist_checkpoint_root_folder + \"/\" + 'hf/' + name\n",
    "if IS_LORA:\n",
    "  model = load_model(model_name, None, True)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "else:\n",
    "  model = load_model(hf_model_path, None, True)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(hf_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "if IS_LORA:\n",
    "  model = PeftModel.from_pretrained(model, output_dir)\n",
    "import random\n",
    "#shuffle eval_messages\n",
    "random.shuffle(eval_messages)\n",
    "\n",
    "for messages in eval_messages[:10]:\n",
    "  print()\n",
    "  print(messages[-1]['content'])\n",
    "  prompt_tokens = tokenizer.apply_chat_template(messages)\n",
    "  prompt_tokens = torch.tensor(prompt_tokens).long()\n",
    "  prompt_tokens = prompt_tokens.unsqueeze(0).to(\"cuda\")\n",
    "  attention_mask = torch.ones_like(prompt_tokens)\n",
    "  output = model.generate(input_ids=prompt_tokens, attention_mask=attention_mask, max_new_tokens=500, use_cache=False, do_sample=True) \n",
    "  print(tokenizer.decode(output[0][len(prompt_tokens[0]):], skip_special_tokens=True).replace('assistant', '').strip())\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 3998 (0a683e80)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/mnt/efs/checkpoints/hf/r2ai-3.2-1B-Instruct.fp16.gguf' to '/mnt/efs/checkpoints/hf/r2ai-3.2-1B-Instruct.Q4_K_M.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 147 tensors from /mnt/efs/checkpoints/hf/r2ai-3.2-1B-Instruct.fp16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = R2Ai 3.2 1B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = r2ai-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  15:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  17:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  18:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,280147]  = [\" \", \" \", \" \", \"...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type  f16:  113 tensors\n",
      "[   1/ 147]                    rope_freqs.weight - [   32,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   2/ 147]                    token_embd.weight - [ 2048, 128256,     1,     1], type =    f16, converting to q6_K .. size =   501.00 MiB ->   205.49 MiB\n",
      "[   3/ 147]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   4/ 147]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[   5/ 147]                blk.0.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   6/ 147]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   7/ 147]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   8/ 147]                  blk.0.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[   9/ 147]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  10/ 147]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  11/ 147]                  blk.0.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  12/ 147]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  13/ 147]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  14/ 147]                blk.1.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  15/ 147]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  16/ 147]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  17/ 147]                  blk.1.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  18/ 147]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  19/ 147]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  20/ 147]                  blk.1.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  21/ 147]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  22/ 147]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  23/ 147]               blk.10.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  24/ 147]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  25/ 147]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  26/ 147]                 blk.10.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  27/ 147]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  28/ 147]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  29/ 147]                 blk.10.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  30/ 147]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  31/ 147]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  32/ 147]               blk.11.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  33/ 147]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  34/ 147]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  35/ 147]                 blk.11.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  36/ 147]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  37/ 147]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  38/ 147]                 blk.11.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  39/ 147]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  40/ 147]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  41/ 147]               blk.12.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  42/ 147]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  43/ 147]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  44/ 147]                 blk.12.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  45/ 147]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  46/ 147]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  47/ 147]                 blk.12.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  48/ 147]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  49/ 147]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  50/ 147]               blk.13.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  51/ 147]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  52/ 147]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  53/ 147]                 blk.13.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  54/ 147]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  55/ 147]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  56/ 147]                 blk.13.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  57/ 147]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  58/ 147]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  59/ 147]               blk.14.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  60/ 147]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  61/ 147]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  62/ 147]                 blk.14.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  63/ 147]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  64/ 147]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  65/ 147]                 blk.14.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  66/ 147]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  67/ 147]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  68/ 147]               blk.15.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  69/ 147]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  70/ 147]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  71/ 147]                 blk.15.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  72/ 147]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  73/ 147]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  74/ 147]                 blk.15.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  75/ 147]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  76/ 147]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  77/ 147]                blk.2.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  78/ 147]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  79/ 147]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  80/ 147]                  blk.2.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  81/ 147]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  82/ 147]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  83/ 147]                  blk.2.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  84/ 147]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  85/ 147]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  86/ 147]                blk.3.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  87/ 147]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  88/ 147]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  89/ 147]                  blk.3.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  90/ 147]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  91/ 147]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[  92/ 147]                  blk.3.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  93/ 147]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  94/ 147]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  95/ 147]                blk.4.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  96/ 147]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  97/ 147]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  98/ 147]                  blk.4.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  99/ 147]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 100/ 147]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 101/ 147]                  blk.4.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 102/ 147]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 103/ 147]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 104/ 147]                blk.5.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 105/ 147]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 106/ 147]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 107/ 147]                  blk.5.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 108/ 147]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 109/ 147]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 110/ 147]                  blk.5.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 111/ 147]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 112/ 147]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 113/ 147]                blk.6.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 114/ 147]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 115/ 147]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 116/ 147]                  blk.6.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 117/ 147]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 118/ 147]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 119/ 147]                  blk.6.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 120/ 147]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 121/ 147]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 122/ 147]                blk.7.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 123/ 147]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 124/ 147]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 125/ 147]                  blk.7.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 126/ 147]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 127/ 147]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 128/ 147]                  blk.7.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 129/ 147]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 130/ 147]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 131/ 147]                blk.8.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 132/ 147]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 133/ 147]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 134/ 147]                  blk.8.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 135/ 147]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 136/ 147]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 137/ 147]                  blk.8.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 138/ 147]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 139/ 147]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 140/ 147]                blk.9.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 141/ 147]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 142/ 147]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 143/ 147]                  blk.9.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 144/ 147]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 145/ 147]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
      "[ 146/ 147]                  blk.9.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 147/ 147]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "llama_model_quantize_internal: model size  =  2357.26 MB\n",
      "llama_model_quantize_internal: quant size  =   762.81 MB\n",
      "\n",
      "main: quantize time = 12277.21 ms\n",
      "main:    total time = 12277.21 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/efs/checkpoints/hf/r2ai-3.2-1B-Instruct.Q4_K_M.gguf'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: fix the notebook PATH env so we can put llama.cpp build here\n",
    "#!python ./llama.cpp/convert_hf_to_gguf.py {hf_model_path} --outtype f16 --outfile {hf_model_path}.fp16.gguf\n",
    "q_method = \"Q4_K_M\"\n",
    "q_path = f\"{hf_model_path}.{q_method}.gguf\"\n",
    "!./llama.cpp/llama-quantize {hf_model_path}.fp16.gguf {q_path} {q_method}\n",
    "q_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 4 CUDA devices:\n",
      "  Device 0: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "  Device 1: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "  Device 2: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "  Device 3: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "build: 3998 (0a683e80) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA A10G) - 18975 MiB free\n",
      "llama_load_model_from_file: using device CUDA1 (NVIDIA A10G) - 18663 MiB free\n",
      "llama_load_model_from_file: using device CUDA2 (NVIDIA A10G) - 18663 MiB free\n",
      "llama_load_model_from_file: using device CUDA3 (NVIDIA A10G) - 21993 MiB free\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 147 tensors from /mnt/efs/checkpoints/hf/r2ai-3.2-1B-Instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = R2Ai 3.2 1B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = r2ai-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  15:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  17:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  18:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,280147]  = [\" \", \" \", \" \", \"...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type q4_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 16\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 1.24 B\n",
      "llm_load_print_meta: model size       = 762.81 MiB (5.18 BPW) \n",
      "llm_load_print_meta: general.name     = R2Ai 3.2 1B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 ''\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/17 layers to GPU\n",
      "llm_load_tensors: CPU_Mapped model buffer size =   762.81 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 131072\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  8850.25 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   260.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 518\n",
      "llama_new_context_with_model: graph splits = 181 (with bs=512), 1 (with bs=1)\n",
      "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 24\n",
      "\n",
      "system_info: n_threads = 24 (n_threads_batch = 24) / 48 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "\n",
      "check_double_bos_eos: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "sampler seed: 3530090391\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
      "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
      "generate: n_ctx = 131072, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 31 Oct 2024\n",
      "\n",
      "user\n",
      "\n",
      "What is the capital of France?assistant\n",
      "\n",
      "Paris. [end of text]\n",
      "\n",
      "\n",
      "llama_perf_sampler_print:    sampling time =       0.27 ms /    46 runs   (    0.01 ms per token, 172284.64 tokens per second)\n",
      "llama_perf_context_print:        load time =    4686.48 ms\n",
      "llama_perf_context_print: prompt eval time =     395.10 ms /    43 tokens (    9.19 ms per token,   108.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =      23.31 ms /     2 runs   (   11.65 ms per token,    85.80 tokens per second)\n",
      "llama_perf_context_print:       total time =     421.66 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 4 CUDA devices:\n",
      "  Device 0: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "  Device 1: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "  Device 2: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "  Device 3: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "build: 3998 (0a683e80) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA A10G) - 18975 MiB free\n",
      "llama_load_model_from_file: using device CUDA1 (NVIDIA A10G) - 18663 MiB free\n",
      "llama_load_model_from_file: using device CUDA2 (NVIDIA A10G) - 18663 MiB free\n",
      "llama_load_model_from_file: using device CUDA3 (NVIDIA A10G) - 21993 MiB free\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 147 tensors from /mnt/efs/checkpoints/hf/r2ai-3.2-1B-Instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = R2Ai 3.2 1B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = r2ai-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  15:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  17:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  18:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,280147]  = [\" \", \" \", \" \", \"...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type q4_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 16\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 1.24 B\n",
      "llm_load_print_meta: model size       = 762.81 MiB (5.18 BPW) \n",
      "llm_load_print_meta: general.name     = R2Ai 3.2 1B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 ''\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/17 layers to GPU\n",
      "llm_load_tensors: CPU_Mapped model buffer size =   762.81 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 131072\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  8850.25 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   260.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 518\n",
      "llama_new_context_with_model: graph splits = 181 (with bs=512), 1 (with bs=1)\n",
      "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 24\n",
      "\n",
      "system_info: n_threads = 24 (n_threads_batch = 24) / 48 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "\n",
      "check_double_bos_eos: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "sampler seed: 54667957\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
      "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
      "generate: n_ctx = 131072, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 31 Oct 2024\n",
      "\n",
      "***RADARE2 MODE: ON***user\n",
      "\n",
      "List all the functionsassistant\n",
      "\n",
      "afl [end of text]\n",
      "\n",
      "\n",
      "llama_perf_sampler_print:    sampling time =       0.27 ms /    51 runs   (    0.01 ms per token, 192452.83 tokens per second)\n",
      "llama_perf_context_print:        load time =    4719.18 ms\n",
      "llama_perf_context_print: prompt eval time =     421.65 ms /    48 tokens (    8.78 ms per token,   113.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =      23.06 ms /     2 runs   (   11.53 ms per token,    86.74 tokens per second)\n",
      "llama_perf_context_print:       total time =     448.04 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 4 CUDA devices:\n",
      "  Device 0: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "  Device 1: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "  Device 2: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "  Device 3: NVIDIA A10G, compute capability 8.6, VMM: yes\n",
      "build: 3998 (0a683e80) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA A10G) - 18975 MiB free\n",
      "llama_load_model_from_file: using device CUDA1 (NVIDIA A10G) - 18663 MiB free\n",
      "llama_load_model_from_file: using device CUDA2 (NVIDIA A10G) - 18663 MiB free\n",
      "llama_load_model_from_file: using device CUDA3 (NVIDIA A10G) - 21993 MiB free\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 147 tensors from /mnt/efs/checkpoints/hf/r2ai-3.2-1B-Instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = R2Ai 3.2 1B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = r2ai-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1B\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  15:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  17:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  18:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,280147]  = [\" \", \" \", \" \", \"...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   34 tensors\n",
      "llama_model_loader: - type q4_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 16\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 1.24 B\n",
      "llm_load_print_meta: model size       = 762.81 MiB (5.18 BPW) \n",
      "llm_load_print_meta: general.name     = R2Ai 3.2 1B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 ''\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/17 layers to GPU\n",
      "llm_load_tensors: CPU_Mapped model buffer size =   762.81 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 131072\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  8850.25 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   260.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 518\n",
      "llama_new_context_with_model: graph splits = 181 (with bs=512), 1 (with bs=1)\n",
      "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 24\n",
      "\n",
      "system_info: n_threads = 24 (n_threads_batch = 24) / 48 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "\n",
      "check_double_bos_eos: Added a BOS token to the prompt as specified by the model but the prompt also starts with a BOS token. So now the final prompt starts with 2 BOS tokens. Are you sure this is what you want?\n",
      "sampler seed: 3675008361\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
      "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
      "generate: n_ctx = 131072, n_batch = 2048, n_predict = -1, n_keep = 1\n",
      "\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 31 Oct 2024\n",
      "\n",
      "***RADARE2 MODE: ON***user\n",
      "\n",
      "disassemble the main functionassistant\n",
      "\n",
      "pd 0x8000 [end of text]\n",
      "\n",
      "\n",
      "llama_perf_sampler_print:    sampling time =       0.57 ms /    56 runs   (    0.01 ms per token, 98591.55 tokens per second)\n",
      "llama_perf_context_print:        load time =    4677.88 ms\n",
      "llama_perf_context_print: prompt eval time =     393.49 ms /    49 tokens (    8.03 ms per token,   124.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =      68.03 ms /     6 runs   (   11.34 ms per token,    88.20 tokens per second)\n",
      "llama_perf_context_print:       total time =     467.55 ms /    55 tokens\n"
     ]
    }
   ],
   "source": [
    "for messages in eval_messages[:3]:\n",
    "  prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "  !./llama.cpp/llama-cli -lv 0 --model {q_path} --prompt \"{prompt}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f8a801051b4f4d957a08bb6fa5c93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "r2ai-3.2-1B-Instruct.Q4_K_M.gguf:   0%|          | 0.00/808M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dnakov/r2ai-3.2-1B-Instruct-GGUF/commit/1038889bdf85e9c590d85b95ef2f3cb17d027149', commit_message='Upload r2ai-3.2-1B-Instruct.Q4_K_M.gguf with huggingface_hub', commit_description='', oid='1038889bdf85e9c590d85b95ef2f3cb17d027149', pr_url=None, repo_url=RepoUrl('https://huggingface.co/dnakov/r2ai-3.2-1B-Instruct-GGUF', endpoint='https://huggingface.co', repo_type='model', repo_id='dnakov/r2ai-3.2-1B-Instruct-GGUF'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "# huggingface_hub.interpreter_login()\n",
    "hf_username = huggingface_hub.whoami()['name']\n",
    "repo_id = f\"{hf_username}/{name}-GGUF\"\n",
    "huggingface_hub.create_repo(repo_id=repo_id)\n",
    "huggingface_hub.upload_file(path_or_fileobj=q_path, path_in_repo=f\"{name}.{q_method}.gguf\", repo_id=repo_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
