{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'r2ai-model'...\n",
      "Warning: Permanently added 'github.com' (ED25519) to the list of known hosts.\n",
      "git@github.com: Permission denied (publickey).\n",
      "fatal: Could not read from remote repository.\n",
      "\n",
      "Please make sure you have the correct access rights\n",
      "and the repository exists.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/daniel/r2ai-model.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a42c1c00bf4b5a95c70fc9b52167bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58f84aa1f034595829cb5528fb6d62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/378 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset(\"json\", data_files=\"r2ai-model/data/radare2/radare2_train.jsonl\", split=\"train\")\n",
    "split = dataset.train_test_split(test_size=0.1)\n",
    "split.save_to_disk('r2_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 3400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 378\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_from_disk('r2_dataset') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting datasource.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile datasource.py\n",
    "\n",
    "import datasets\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "EOT_ID = 128009\n",
    "\n",
    "def mask_target(target,seq):\n",
    "    for i in range(len(seq)-len(target)):\n",
    "        if seq[i:i+len(target)] == target:\n",
    "            seq[i:i+len(target)] = [-100] * len(target)\n",
    "    return seq\n",
    "\n",
    "def get_custom_dataset(dataset_config, tokenizer, split):\n",
    "    \n",
    "    def tokenize_function(messages):\n",
    "        dialog_tokens = tokenizer.apply_chat_template(messages)\n",
    "        eot_indices = [i for i,n in enumerate(dialog_tokens) if n == EOT_ID]\n",
    "        labels = copy.copy(dialog_tokens)\n",
    "        system_or_user = (tokenizer.encode(\"system\")[-1], tokenizer.encode(\"user\")[-1])\n",
    "        labels[0] = -100 # bos token\n",
    "        last_idx = 1\n",
    "        for n, idx in enumerate(eot_indices):\n",
    "            role_token = labels[last_idx+1]\n",
    "            if role_token in system_or_user:\n",
    "                # Set labels to -100 for system and user tokens to ignore in loss function\n",
    "                labels[last_idx:idx+1] = [-100] * (idx-last_idx+1)\n",
    "            last_idx = idx + 1\n",
    "        mask_target(tokenizer.encode(\"<|start_header_id|>assistant<|end_header_id|>\", add_special_tokens=False), labels)\n",
    "        dialog_tokens = [dialog_tokens]\n",
    "        labels_tokens = [labels]\n",
    "        combined_tokens = {\n",
    "            \"input_ids\": list(itertools.chain(*(t for t in dialog_tokens))),\n",
    "            \"labels\": list(itertools.chain(*(t for t in labels_tokens))),\n",
    "        }\n",
    "    \n",
    "        return dict(combined_tokens, attention_mask=[1]*len(combined_tokens[\"input_ids\"]))\n",
    "\n",
    "    dataset = datasets.load_from_disk('r2_dataset')\n",
    "    if split == 'train':\n",
    "        dataset = dataset['train']\n",
    "    else:\n",
    "        dataset = dataset['test']\n",
    "    dataset = dataset.map(lambda x: tokenize_function(x['messages']), remove_columns=['messages'])\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing finetuning.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile finetuning.py\n",
    "\n",
    "import fire\n",
    "from llama_recipes.finetuning import main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1031 16:08:33.721000 125173606377280 torch/distributed/run.py:779] \n",
      "W1031 16:08:33.721000 125173606377280 torch/distributed/run.py:779] *****************************************\n",
      "W1031 16:08:33.721000 125173606377280 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1031 16:08:33.721000 125173606377280 torch/distributed/run.py:779] *****************************************\n",
      "/home/ubuntu/.local/lib/python3.11/site-packages/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
      "  from torch.distributed._shard.checkpoint import (\n",
      "/home/ubuntu/.local/lib/python3.11/site-packages/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
      "  from torch.distributed._shard.checkpoint import (\n",
      "/home/ubuntu/.local/lib/python3.11/site-packages/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
      "  from torch.distributed._shard.checkpoint import (\n",
      "/home/ubuntu/.local/lib/python3.11/site-packages/llama_recipes/model_checkpointing/checkpoint_handler.py:17: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
      "  from torch.distributed._shard.checkpoint import (\n",
      "Clearing GPU cache for all ranks\n",
      "--> Running with torch dist debug set to detail\n",
      "--> Model meta-llama/Llama-3.2-1B-Instruct\n",
      "\n",
      "--> meta-llama/Llama-3.2-1B-Instruct has 1235.8144 Million params\n",
      "\n",
      "bFloat16 enabled for mixed precision - using bfSixteen policy\n",
      "--> applying fsdp activation checkpointing...\n",
      "--> Training Set Length = 3400\n",
      "--> applying fsdp activation checkpointing...\n",
      "--> applying fsdp activation checkpointing...\n",
      "--> Validation Set Length = 378\n",
      "--> applying fsdp activation checkpointing...\n",
      "length of dataset_train 3400\n",
      "Can not find the custom data_collator in the dataset.py file (datasource.py).\n",
      "Using the default data_collator instead.\n",
      "--> Num of Training Set Batches loaded = 212\n",
      "--> Num of Validation Set Batches loaded = 94\n",
      "--> Num of Validation Set Batches loaded = 94\n",
      "Starting epoch 0/3\n",
      "train_config.max_train_step: 0\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m                                \u001b[0m| 0/212 [00:00<?, ?it/s]\u001b[0mlength of dataset_train 3400\n",
      "Can not find the custom data_collator in the dataset.py file (datasource.py).\n",
      "Using the default data_collator instead.\n",
      "--> Num of Training Set Batches loaded = 212\n",
      "length of dataset_train 3400\n",
      "Can not find the custom data_collator in the dataset.py file (datasource.py).\n",
      "Using the default data_collator instead.\n",
      "--> Num of Training Set Batches loaded = 212\n",
      "--> Num of Validation Set Batches loaded = 94\n",
      "--> Num of Validation Set Batches loaded = 94\n",
      "Starting epoch 0/3\n",
      "train_config.max_train_step: 0\n",
      "--> Num of Validation Set Batches loaded = 94\n",
      "--> Num of Validation Set Batches loaded = 94\n",
      "Starting epoch 0/3\n",
      "train_config.max_train_step: 0\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m                                \u001b[0m| 0/212 [00:00<?, ?it/s]\u001b[0m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m                                \u001b[0m| 0/212 [00:00<?, ?it/s]\u001b[0mlength of dataset_train 3400\n",
      "Can not find the custom data_collator in the dataset.py file (datasource.py).\n",
      "Using the default data_collator instead.\n",
      "--> Num of Training Set Batches loaded = 212\n",
      "--> Num of Validation Set Batches loaded = 94\n",
      "--> Num of Validation Set Batches loaded = 94\n",
      "Starting epoch 0/3\n",
      "train_config.max_train_step: 0\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/cuda/memory.py:343: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m                                \u001b[0m| 0/212 [00:00<?, ?it/s]\u001b[0m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "Training Epoch: 1/3, step 53/212 completed (loss: 0.7097226977348328):  25%|\u001b[34mâ–Ž\u001b[0m| 5\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!TOKENIZER_PARALLELISM=1 torchrun --nnodes 1 --nproc_per_node 4 finetuning.py \\\\\n",
    "  --enable_fsdp \\\\\n",
    "  --model_name meta-llama/Llama-3.2-1B-Instruct \\\\\n",
    "  --dist_checkpoint_root_folder model_checkpoints \\\\\n",
    "  --dist_checkpoint_folder fine-tuned \\\\\n",
    "  --fsdp_config.pure_bf16 \\\\\n",
    "  --use_fast_kernels \\\\\n",
    "  --dataset \"custom_dataset\" \\\\\n",
    "  --custom_dataset.file \"datasource.py\" \\\\\n",
    "  --batching_strategy \"padding\"\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
